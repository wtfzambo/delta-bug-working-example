{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfc1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7324c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = pyspark.sql.SparkSession.builder.appName('access_logs_upserts_test') \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8733c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/07 18:13:16 WARN Utils: Your hostname, Zambo-ROG resolves to a loopback address: 127.0.1.1; using 172.24.112.98 instead (on interface eth0)\n",
      "23/01/07 18:13:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/wtfzambo/my-stuff/delta_bug/.venv/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/wtfzambo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/wtfzambo/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-feb44f56-5e53-4938-8060-9c98f123b67e;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/wtfzambo/my-stuff/delta_bug/.venv/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 232ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-feb44f56-5e53-4938-8060-9c98f123b67e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/9ms)\n",
      "23/01/07 18:13:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5395df",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "col_a string,\n",
    "col_b integer\n",
    "\"\"\"\n",
    "\n",
    "df = spark.read.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('escape', '\"') \\\n",
    "    .schema(schema) \\\n",
    "    .load('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e81e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|col_a|col_b|\n",
      "+-----+-----+\n",
      "|  foo|    1|\n",
      "|  bar|    2|\n",
      "|  baz|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a7b6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = 'delta-table'\n",
    "abs_delta_path = os.path.abspath(delta_path)\n",
    "my_delta_table = f'delta.`{abs_delta_path}`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9dd5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(1)\n",
    "df.write \\\n",
    "    .format('delta') \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('overwriteSchema', 'true') \\\n",
    "    .save(abs_delta_path)\n",
    "\n",
    "dt = DeltaTable.forPath(spark, abs_delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dc3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|          col_a|   string|       |\n",
      "|          col_b|      int|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE TABLE {my_delta_table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfef8189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"ALTER TABLE {my_delta_table} ADD COLUMNS (my_new_col string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "900aa4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|          col_a|   string|       |\n",
      "|          col_b|      int|       |\n",
      "|     my_new_col|   string|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE TABLE {my_delta_table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa94e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------\n",
      " version             | 5                                                                                             \n",
      " timestamp           | 2023-01-07 18:13:30.014                                                                       \n",
      " userId              | null                                                                                          \n",
      " userName            | null                                                                                          \n",
      " operation           | ADD COLUMNS                                                                                   \n",
      " operationParameters | {columns -> [{\"column\":{\"name\":\"my_new_col\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}}]} \n",
      " job                 | null                                                                                          \n",
      " notebook            | null                                                                                          \n",
      " clusterId           | null                                                                                          \n",
      " readVersion         | 4                                                                                             \n",
      " isolationLevel      | null                                                                                          \n",
      " isBlindAppend       | true                                                                                          \n",
      " operationMetrics    | {}                                                                                            \n",
      " userMetadata        | null                                                                                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.history(1).show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9968e",
   "metadata": {},
   "source": [
    "As you can see, the new column seems to be added successfully. However this is not reflected in the `dt` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d25d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_a: string (nullable = true)\n",
      " |-- col_b: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a87aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|col_a|col_b|\n",
      "+-----+-----+\n",
      "|  foo|    1|\n",
      "|  bar|    2|\n",
      "|  baz|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b68e82",
   "metadata": {},
   "source": [
    "On the other hand, running something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e25b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"UPDATE {my_delta_table} SET col_a = 'bananas'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc69b6",
   "metadata": {},
   "source": [
    "Is reflected in the `dt` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a31d902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  col_a|col_b|\n",
      "+-------+-----+\n",
      "|bananas|    1|\n",
      "|bananas|    2|\n",
      "|bananas|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac13a8",
   "metadata": {},
   "source": [
    "The only way to see the new column is to re-create the `dt` object by running again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc02fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DeltaTable.forPath(spark, abs_delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23d701c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n",
      "|  col_a|col_b|my_new_col|\n",
      "+-------+-----+----------+\n",
      "|bananas|    1|      null|\n",
      "|bananas|    2|      null|\n",
      "|bananas|    3|      null|\n",
      "+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e5006",
   "metadata": {},
   "source": [
    "A similar behavior can be seen if casting the spark Dataframe to a Pandas dataframe after updating a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6141f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"UPDATE {my_delta_table} SET my_new_col = 'mangos'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f064c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n",
      "|  col_a|col_b|my_new_col|\n",
      "+-------+-----+----------+\n",
      "|bananas|    1|    mangos|\n",
      "|bananas|    2|    mangos|\n",
      "|bananas|    3|    mangos|\n",
      "+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No problem\n",
    "\n",
    "dt.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e46929f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_a</th>\n",
       "      <th>col_b</th>\n",
       "      <th>my_new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bananas</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bananas</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bananas</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     col_a  col_b my_new_col\n",
       "0  bananas      1       None\n",
       "1  bananas      2       None\n",
       "2  bananas      3       None"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latest update to `my_new_col` is missing\n",
    "\n",
    "dt.toDF().toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
